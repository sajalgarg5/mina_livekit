<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <title>Recall Bot Audio - Stable Stream</title>
    <style>
        body { 
            display: flex; flex-direction: column; justify-content: center; align-items: center; 
            height: 100vh; margin: 0; background: #000033; font-family: sans-serif; color: white; 
            overflow: hidden;
        }
        .status-indicator { 
            width: 100px; height: 100px; border-radius: 50%; background-color: #333; 
            border: 4px solid rgba(255,255,255,0.1); transition: background-color 0.2s; 
        }
        .speaking { background-color: #00d2ff; box-shadow: 0 0 30px #00d2ff; }
        #statusText { margin-top: 20px; font-weight: bold; color: #82baf7; text-transform: uppercase; letter-spacing: 1px; }
        #log { 
            margin-top: 20px; font-size: 10px; width: 90%; height: 80px; 
            overflow-y: auto; background: rgba(0,0,0,0.4); padding: 10px; 
            border-radius: 5px; font-family: monospace; color: #aaa;
        }
    </style>
</head>
<body>
    <div class="status-indicator" id="statusIndicator"></div>
    <div id="statusText">Awaiting Interaction...</div>
    <div id="log"></div>

<script type="module">
    import { AudioAnalysis } from './audio_analysis.js';

    // Settings optimized for Recall.ai Bot environments
    const BASE_URL = "https://tel-retreat-pure-education.trycloudflare.com"; 
    const SAMPLE_RATE = 16000;

    let audioCtx = null;
    let streamNode = null;
    let analyser = null;
    let leftoverByte = null;

    function log(msg) {
        const el = document.getElementById("log");
        const div = document.createElement("div");
        div.textContent = `[${new Date().toLocaleTimeString()}] ${msg}`;
        el.appendChild(div);
        el.scrollTop = el.scrollHeight;
    }

    /**
     * Initializes the Audio Stack with a 'playback' latency hint.
     * This tells the bot's browser to use a larger internal hardware buffer.
     */
    async function initAudio() {
        if (audioCtx) return;
        
        try {
            audioCtx = new (window.AudioContext || window.webkitAudioContext)({ 
                sampleRate: SAMPLE_RATE,
                latencyHint: 'playback' 
            });

            await audioCtx.audioWorklet.addModule('stream_processor.js');
            
            streamNode = new AudioWorkletNode(audioCtx, 'stream_processor');
            analyser = audioCtx.createAnalyser();
            analyser.fftSize = 512; // Smaller FFT for less CPU usage
            
            streamNode.connect(analyser);
            analyser.connect(audioCtx.destination);
            
            await audioCtx.resume();
            
            log("‚úÖ Audio Worklet Ready");
            document.getElementById("statusText").textContent = "Stream Active";
            
            connectSSE();
            startVisualizerLoop();
        } catch (e) {
            log("‚ùå Init Error: " + e.message);
        }
    }

    /**
     * Handles the Base64 chunks and manages byte-alignment.
     */
    function handleAudioData(base64) {
        if (!streamNode) return;

        try {
            const binary = atob(base64);
            let bytes = new Uint8Array(binary.length);
            for (let i = 0; i < binary.length; i++) bytes[i] = binary.charCodeAt(i);

            // Stitch partial PCM samples (Protects against static noise)
            if (leftoverByte !== null) {
                let stitched = new Uint8Array(bytes.length + 1);
                stitched[0] = leftoverByte;
                stitched.set(bytes, 1);
                bytes = stitched;
                leftoverByte = null;
            }
            if (bytes.length % 2 !== 0) {
                leftoverByte = bytes[bytes.length - 1];
                bytes = bytes.slice(0, bytes.length - 1);
            }

            const float32 = new Float32Array(bytes.length / 2);
            const view = new DataView(bytes.buffer);
            for (let i = 0; i < float32.length; i++) {
                float32[i] = view.getInt16(i * 2, true) / 32768.0;
            }

            // Feed the thread-isolated circular buffer
            streamNode.port.postMessage({ event: 'write', buffer: float32 });
        } catch (err) {
            console.error("PCM Decode Error:", err);
        }
    }

    function connectSSE() {
        const ev = new EventSource(`${BASE_URL}/listen_audio`);
        
        ev.addEventListener("audio", (e) => handleAudioData(e.data));
        
        ev.onopen = () => log("üîå SSE Connected to Bot Source");
        ev.onerror = () => {
            log("üì° Connection lost, retrying...");
            ev.close();
            setTimeout(connectSSE, 2000);
        };
    }

    /**
     * Simplified visualizer loop to save Bot CPU cycles.
     */
    function startVisualizerLoop() {
        const indicator = document.getElementById("statusIndicator");
        
        function update() {
            requestAnimationFrame(update);
            if (!analyser) return;

            // Use your provided AudioAnalysis class
            const data = AudioAnalysis.getFrequencies(analyser, SAMPLE_RATE, null, 'voice');
            const vol = data.values.reduce((a, b) => a + b, 0) / data.values.length;
            
            if (vol > 0.04) {
                indicator.classList.add("speaking");
            } else {
                indicator.classList.remove("speaking");
            }
        }
        update();
    }
    
    function startDelayedAudio(delayMs) {
    log(`‚è≥ Waiting ${delayMs}ms before starting audio engine...`);
    
    setTimeout(async () => {
        try {
            await initAudio();
            log("üöÄ Delayed initialization complete.");
        } catch (err) {
            log("‚ùå Delayed init failed: " + err.message);
        }
    }, delayMs);
}
    // Interaction is mandatory for Web Audio to start
    window.onload = () => {
    startDelayedAudio(5000); 
};
</script>
</body>
</html>